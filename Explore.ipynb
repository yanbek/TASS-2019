{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load needed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "from spellchecker import SpellChecker\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ftfy\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from polyglot.text import Text\n",
    "from mtranslate import translate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = namedtuple('Tweet', ['tweetid', 'content', 'polarity'])\n",
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "nlp = spacy.load('es')\n",
    "spell = SpellChecker(language='es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from given file and return it as a dataframe.\"\"\"\n",
    "    tweets: List = []\n",
    "    with open(file, 'r') as f:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            tweets.append(tweet(child[0].text, child[2].text, child[5][0][0].text))\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "def read_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from given folder, combines the training and dev set\n",
    "    and return them combined as a dataframe.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    files = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    for file in files:\n",
    "        if 'xml' in file:\n",
    "            dataframes.append(read_data(folder + file))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "\n",
    "Don't need to care for emoticons, because there are less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(tweet: str) -> str:\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', tweet)\n",
    "\n",
    "def lower_case(tweet: str) -> str:\n",
    "    \"\"\"Turn a tweet to lower case.\"\"\"\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_question_mark(tweet: str) -> str:\n",
    "    \"\"\"Remove spanish question mark from a tweet.\"\"\"\n",
    "    return tweet.replace('¿', '')\n",
    "\n",
    "def remove_punctuation(tweet: str) -> str:\n",
    "    \"\"\"Remove punctuation from a tweet.\"\"\"\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_whitespace(tweet: str) -> str:\n",
    "    return tweet.strip()\n",
    "\n",
    "def check_int(s):\n",
    "    if s[0] in ('-', '+'):\n",
    "        return s[1:].isdigit()\n",
    "    return s.isdigit()\n",
    "\n",
    "def remove_numbers(tweet: str) -> str:\n",
    "    \"\"\"Remove numbers from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if not check_int(word)])\n",
    "\n",
    "def fix_encoding(tweet: str) -> str:\n",
    "    return ftfy.fix_encoding(tweet)\n",
    "\n",
    "def fix_repeated_letters(tweet: str) -> str:\n",
    "    \"\"\"Replace repeated characters (3 repetitions or more) with only two characters.\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "def fix_spelling(tweet: str) -> str:\n",
    "    \"\"\"Fix spelling error in tweets.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(tokenized)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] in misspelled:\n",
    "            tokenized[i] = spell.correction(tokenized[i])\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_mention,\n",
    "                 lower_case,\n",
    "                 remove_question_mark,\n",
    "                 remove_punctuation,\n",
    "                 remove_numbers,\n",
    "                 remove_whitespace,\n",
    "                 fix_repeated_letters,\n",
    "                 fix_encoding,\n",
    "                 fix_spelling\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling\n",
    "Preprocessing that should be done after spell correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet: str) -> str:\n",
    "    \"\"\"Remove stopwords from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if word not in stopWords])\n",
    "\n",
    "def stem_tweet(tweet: str) -> str:\n",
    "    tweet = nlp(tweet)\n",
    "    return ' '.join([token.lemma_ for token in tweet])\n",
    "\n",
    "\n",
    "def clean_tweet2(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_stopwords,\n",
    "                 stem_tweet\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df):\n",
    "    new_trans = 2\n",
    "    new_df = df.copy()\n",
    "    new_df['augmented'] = 0\n",
    "    for index, row in tqdm(new_df.iterrows()):\n",
    "        trans_en = translate(translate(row['content'], 'en', 'es'), 'es', 'en')\n",
    "\n",
    "        new_row_en = {'tweetid': row['tweetid'],\n",
    "                      'content': trans_en,\n",
    "                      'polarity': row['polarity'],\n",
    "                      'augmented': 1}\n",
    "\n",
    "        new_df = new_df.append(new_row_en, ignore_index=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 618/777 [07:17<02:21,  1.13it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54a011dd9e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/cr/intertass_cr_train.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/cr/cleaned.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# Close bar and return pandas calculation result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-42c46521f53c>\u001b[0m in \u001b[0;36mclean_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     58\u001b[0m                  ]\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-42c46521f53c>\u001b[0m in \u001b[0;36mfix_spelling\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmisspelled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mcorrection\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 str: The most likely candidate \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    159\u001b[0m         return set(\n\u001b[1;32m    160\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TASS-2019-ZSQ9DPWS/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m_check_if_should_check\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# check if it is a number (int, float, etc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = read_data(\"data/cr/intertass_cr_train.xml\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet)\n",
    "df.to_csv(\"data/cr/cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cr/cleaned.csv\")\n",
    "df = augment_data(df)\n",
    "df.to_csv(\"data/cr/cleaned_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1554 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 17/1554 [00:00<00:09, 162.31it/s]\u001b[A\n",
      "  2%|▏         | 34/1554 [00:00<00:09, 162.16it/s]\u001b[A\n",
      "  3%|▎         | 51/1554 [00:00<00:09, 163.34it/s]\u001b[A\n",
      "  4%|▍         | 68/1554 [00:00<00:09, 163.55it/s]\u001b[A\n",
      "  6%|▌         | 86/1554 [00:00<00:08, 166.63it/s]\u001b[A\n",
      "  7%|▋         | 103/1554 [00:00<00:08, 166.61it/s]\u001b[A\n",
      "  8%|▊         | 120/1554 [00:00<00:08, 165.00it/s]\u001b[A\n",
      "  9%|▉         | 138/1554 [00:00<00:08, 167.26it/s]\u001b[A\n",
      " 10%|█         | 156/1554 [00:00<00:08, 168.76it/s]\u001b[A\n",
      " 11%|█         | 173/1554 [00:01<00:08, 164.47it/s]\u001b[A\n",
      " 12%|█▏        | 190/1554 [00:01<00:08, 165.59it/s]\u001b[A\n",
      " 13%|█▎        | 207/1554 [00:01<00:08, 166.08it/s]\u001b[A\n",
      " 14%|█▍        | 224/1554 [00:01<00:08, 161.53it/s]\u001b[A\n",
      " 16%|█▌        | 241/1554 [00:01<00:08, 155.02it/s]\u001b[A\n",
      " 17%|█▋        | 257/1554 [00:01<00:08, 152.79it/s]\u001b[A\n",
      " 18%|█▊        | 273/1554 [00:01<00:08, 150.16it/s]\u001b[A\n",
      " 19%|█▊        | 290/1554 [00:01<00:08, 155.25it/s]\u001b[A\n",
      " 20%|█▉        | 307/1554 [00:01<00:07, 158.57it/s]\u001b[A\n",
      " 21%|██        | 323/1554 [00:02<00:07, 157.87it/s]\u001b[A\n",
      " 22%|██▏       | 340/1554 [00:02<00:07, 159.76it/s]\u001b[A\n",
      " 23%|██▎       | 357/1554 [00:02<00:07, 161.24it/s]\u001b[A\n",
      " 24%|██▍       | 374/1554 [00:02<00:07, 162.09it/s]\u001b[A\n",
      " 25%|██▌       | 391/1554 [00:02<00:07, 159.41it/s]\u001b[A\n",
      " 26%|██▋       | 408/1554 [00:02<00:07, 160.65it/s]\u001b[A\n",
      " 27%|██▋       | 425/1554 [00:02<00:06, 162.84it/s]\u001b[A\n",
      " 28%|██▊       | 442/1554 [00:02<00:06, 162.35it/s]\u001b[A\n",
      " 30%|██▉       | 461/1554 [00:02<00:06, 168.01it/s]\u001b[A\n",
      " 31%|███       | 478/1554 [00:02<00:06, 161.97it/s]\u001b[A\n",
      " 32%|███▏      | 495/1554 [00:03<00:07, 149.59it/s]\u001b[A\n",
      " 33%|███▎      | 511/1554 [00:03<00:06, 150.61it/s]\u001b[A\n",
      " 34%|███▍      | 527/1554 [00:03<00:06, 152.87it/s]\u001b[A\n",
      " 35%|███▌      | 544/1554 [00:03<00:06, 157.45it/s]\u001b[A\n",
      " 36%|███▌      | 562/1554 [00:03<00:06, 162.97it/s]\u001b[A\n",
      " 37%|███▋      | 581/1554 [00:03<00:05, 168.47it/s]\u001b[A\n",
      " 39%|███▊      | 600/1554 [00:03<00:05, 172.81it/s]\u001b[A\n",
      " 40%|███▉      | 620/1554 [00:03<00:05, 178.77it/s]\u001b[A\n",
      " 41%|████      | 639/1554 [00:03<00:05, 179.63it/s]\u001b[A\n",
      " 42%|████▏     | 658/1554 [00:04<00:04, 179.79it/s]\u001b[A\n",
      " 44%|████▎     | 677/1554 [00:04<00:04, 178.92it/s]\u001b[A\n",
      " 45%|████▍     | 695/1554 [00:04<00:04, 175.38it/s]\u001b[A\n",
      " 46%|████▌     | 713/1554 [00:04<00:04, 176.40it/s]\u001b[A\n",
      " 47%|████▋     | 732/1554 [00:04<00:04, 179.95it/s]\u001b[A\n",
      " 48%|████▊     | 751/1554 [00:04<00:04, 182.38it/s]\u001b[A\n",
      " 50%|████▉     | 770/1554 [00:04<00:04, 183.99it/s]\u001b[A\n",
      " 51%|█████     | 789/1554 [00:04<00:04, 180.56it/s]\u001b[A\n",
      " 52%|█████▏    | 808/1554 [00:04<00:04, 173.18it/s]\u001b[A\n",
      " 53%|█████▎    | 826/1554 [00:04<00:04, 172.09it/s]\u001b[A\n",
      " 54%|█████▍    | 844/1554 [00:05<00:04, 167.54it/s]\u001b[A\n",
      " 55%|█████▌    | 861/1554 [00:05<00:04, 161.60it/s]\u001b[A\n",
      " 56%|█████▋    | 878/1554 [00:05<00:04, 160.80it/s]\u001b[A\n",
      " 58%|█████▊    | 895/1554 [00:05<00:04, 158.34it/s]\u001b[A\n",
      " 59%|█████▊    | 912/1554 [00:05<00:04, 159.94it/s]\u001b[A\n",
      " 60%|█████▉    | 929/1554 [00:05<00:03, 157.81it/s]\u001b[A\n",
      " 61%|██████    | 945/1554 [00:05<00:03, 154.01it/s]\u001b[A\n",
      " 62%|██████▏   | 961/1554 [00:05<00:03, 148.88it/s]\u001b[A\n",
      " 63%|██████▎   | 976/1554 [00:05<00:03, 147.79it/s]\u001b[A\n",
      " 64%|██████▍   | 992/1554 [00:06<00:03, 151.21it/s]\u001b[A\n",
      " 65%|██████▍   | 1008/1554 [00:06<00:03, 153.56it/s]\u001b[A\n",
      " 66%|██████▌   | 1025/1554 [00:06<00:03, 157.26it/s]\u001b[A\n",
      " 67%|██████▋   | 1042/1554 [00:06<00:03, 160.05it/s]\u001b[A\n",
      " 68%|██████▊   | 1059/1554 [00:06<00:03, 161.37it/s]\u001b[A\n",
      " 69%|██████▉   | 1076/1554 [00:06<00:02, 160.69it/s]\u001b[A\n",
      " 70%|███████   | 1093/1554 [00:06<00:02, 162.59it/s]\u001b[A\n",
      " 71%|███████▏  | 1111/1554 [00:06<00:02, 167.10it/s]\u001b[A\n",
      " 73%|███████▎  | 1128/1554 [00:06<00:02, 166.31it/s]\u001b[A\n",
      " 74%|███████▎  | 1146/1554 [00:06<00:02, 168.69it/s]\u001b[A\n",
      " 75%|███████▍  | 1164/1554 [00:07<00:02, 171.73it/s]\u001b[A\n",
      " 76%|███████▌  | 1182/1554 [00:07<00:02, 170.97it/s]\u001b[A\n",
      " 77%|███████▋  | 1200/1554 [00:07<00:02, 169.39it/s]\u001b[A\n",
      " 78%|███████▊  | 1217/1554 [00:07<00:01, 168.91it/s]\u001b[A\n",
      " 79%|███████▉  | 1234/1554 [00:07<00:01, 165.28it/s]\u001b[A\n",
      " 81%|████████  | 1251/1554 [00:07<00:01, 161.47it/s]\u001b[A\n",
      " 82%|████████▏ | 1269/1554 [00:07<00:01, 164.80it/s]\u001b[A\n",
      " 83%|████████▎ | 1286/1554 [00:07<00:01, 165.46it/s]\u001b[A\n",
      " 84%|████████▍ | 1304/1554 [00:07<00:01, 168.96it/s]\u001b[A\n",
      " 85%|████████▌ | 1322/1554 [00:08<00:01, 170.16it/s]\u001b[A\n",
      " 86%|████████▌ | 1340/1554 [00:08<00:01, 164.68it/s]\u001b[A\n",
      " 87%|████████▋ | 1357/1554 [00:08<00:01, 161.04it/s]\u001b[A\n",
      " 88%|████████▊ | 1374/1554 [00:08<00:01, 162.17it/s]\u001b[A\n",
      " 90%|████████▉ | 1391/1554 [00:08<00:00, 163.55it/s]\u001b[A\n",
      " 91%|█████████ | 1409/1554 [00:08<00:00, 167.27it/s]\u001b[A\n",
      " 92%|█████████▏| 1427/1554 [00:08<00:00, 170.37it/s]\u001b[A\n",
      " 93%|█████████▎| 1447/1554 [00:08<00:00, 176.02it/s]\u001b[A\n",
      " 94%|█████████▍| 1465/1554 [00:08<00:00, 176.81it/s]\u001b[A\n",
      " 95%|█████████▌| 1484/1554 [00:08<00:00, 178.26it/s]\u001b[A\n",
      " 97%|█████████▋| 1503/1554 [00:09<00:00, 180.98it/s]\u001b[A\n",
      " 98%|█████████▊| 1522/1554 [00:09<00:00, 183.03it/s]\u001b[A\n",
      " 99%|█████████▉| 1541/1554 [00:09<00:00, 182.70it/s]\u001b[A\n",
      "100%|██████████| 1554/1554 [00:09<00:00, 166.03it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cr/cleaned_augmented.csv\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet2)\n",
    "df = df.drop_duplicates('content', keep='first')\n",
    "df.to_csv(\"data/cr/cleaned_augmented_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768225400254111744</td>\n",
       "      <td>totalmente puntual</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>770077064833671168</td>\n",
       "      <td>hola sandra habia desear feliz dia madre tarda...</td>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771207534342320128</td>\n",
       "      <td>si andar hacer mejor quedar calladita jaja asi...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>771900763987513345</td>\n",
       "      <td>pereza querer chocar banano</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772550560998301697</td>\n",
       "      <td>bueno mayor cuánto campar tú sos cartaguito ca...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid                                            content  \\\n",
       "0  768225400254111744                                 totalmente puntual   \n",
       "1  770077064833671168  hola sandra habia desear feliz dia madre tarda...   \n",
       "2  771207534342320128  si andar hacer mejor quedar calladita jaja asi...   \n",
       "3  771900763987513345                        pereza querer chocar banano   \n",
       "4  772550560998301697  bueno mayor cuánto campar tú sos cartaguito ca...   \n",
       "\n",
       "  polarity  augmented  \n",
       "0     NONE          0  \n",
       "1        P          0  \n",
       "2        N          0  \n",
       "3        N          0  \n",
       "4        N          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>814853214050906112</td>\n",
       "      <td>Quiero tener carro hacer rápido .</td>\n",
       "      <td>NEU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>815439552038977537</td>\n",
       "      <td>Tristeza hp siempre dar nuevo año .</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>817864316372348928</td>\n",
       "      <td>No encontrar ninguno bueno seriar ninguno</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>817069708927266816</td>\n",
       "      <td>suerte alfa feliz año</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>817032173689696257</td>\n",
       "      <td>si entendi tweet anterior</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweetid                                    content polarity  \\\n",
       "1549  814853214050906112          Quiero tener carro hacer rápido .      NEU   \n",
       "1550  815439552038977537        Tristeza hp siempre dar nuevo año .        N   \n",
       "1551  817864316372348928  No encontrar ninguno bueno seriar ninguno        N   \n",
       "1552  817069708927266816                      suerte alfa feliz año        P   \n",
       "1553  817032173689696257                  si entendi tweet anterior     NONE   \n",
       "\n",
       "      augmented  \n",
       "1549          1  \n",
       "1550          1  \n",
       "1551          1  \n",
       "1552          1  \n",
       "1553          1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1513 entries, 0 to 1553\n",
      "Data columns (total 4 columns):\n",
      "tweetid      1513 non-null int64\n",
      "content      1513 non-null object\n",
      "polarity     1513 non-null object\n",
      "augmented    1513 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 59.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_counts = Counter(df.polarity.values)\n",
    "plotdf = pd.DataFrame.from_dict(polarity_counts, orient='index')\n",
    "plotdf.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn tweets into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist([word for tweet in df.content for word in nltk.word_tokenize(tweet)])\n",
    "word_features = list(all_words) # [_document-classify-all-words]\n",
    "\n",
    "def document_features(document): # [_document-classify-extractor]\n",
    "    document_words = set(nltk.word_tokenize(document)) # [_document-classify-set]\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "        \n",
    "    text = Text(document)\n",
    "    text.language = 'es'\n",
    "    features['positive'] = sum([1 for w in text.words if w.polarity > 0])\n",
    "    features['neutral'] = sum([1 for w in text.words if w.polarity == 0])\n",
    "    features['negative'] = sum([1 for w in text.words if w.polarity < 0])\n",
    "    \n",
    "    return features\n",
    "df_sub = df\n",
    "featuresets = [(document_features(d), c) for (d,c) in zip(df_sub.content, df_sub.polarity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5001831501831502\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "kf = KFold(n_splits=n, shuffle=True)\n",
    "total = 0\n",
    "for train, test in kf.split(featuresets):\n",
    "    classifier = SklearnClassifier(LogisticRegression()).train(np.array(featuresets)[train])\n",
    "    total += nltk.classify.accuracy(classifier, (np.array(featuresets)[test]))\n",
    "print(\"Accuracy:\", total / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SklearnClassifier(LogisticRegression()).train(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384615384615384"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"data/cr/test_cleaned.csv\")\n",
    "featuresets_test = [(document_features(d), c) for (d,c) in zip(df_test.content, df_test.polarity)]\n",
    "nltk.classify.accuracy(classifier, featuresets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TASS-2019",
   "language": "python",
   "name": "tass-2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
