{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load needed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "from spellchecker import SpellChecker\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ftfy\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from polyglot.text import Text\n",
    "from mtranslate import translate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = namedtuple('Tweet', ['tweetid', 'content', 'polarity'])\n",
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "nlp = spacy.load('es')\n",
    "spell = SpellChecker(language='es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from given file and return it as a dataframe.\"\"\"\n",
    "    tweets: List = []\n",
    "    with open(file, 'r') as f:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            tweets.append(tweet(child[0].text, child[2].text, child[5][0][0].text))\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "def read_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from given folder, combines the training and dev set\n",
    "    and return them combined as a dataframe.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    files = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    for file in files:\n",
    "        if 'xml' in file:\n",
    "            dataframes.append(read_data(folder + file))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "\n",
    "Don't need to care for emoticons, because there are less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(tweet: str) -> str:\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', tweet)\n",
    "\n",
    "def lower_case(tweet: str) -> str:\n",
    "    \"\"\"Turn a tweet to lower case.\"\"\"\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_question_mark(tweet: str) -> str:\n",
    "    \"\"\"Remove spanish question mark from a tweet.\"\"\"\n",
    "    return tweet.replace('Â¿', '')\n",
    "\n",
    "def remove_punctuation(tweet: str) -> str:\n",
    "    \"\"\"Remove punctuation from a tweet.\"\"\"\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_whitespace(tweet: str) -> str:\n",
    "    return tweet.strip()\n",
    "\n",
    "def check_int(s):\n",
    "    if s[0] in ('-', '+'):\n",
    "        return s[1:].isdigit()\n",
    "    return s.isdigit()\n",
    "\n",
    "def remove_numbers(tweet: str) -> str:\n",
    "    \"\"\"Remove numbers from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if not check_int(word)])\n",
    "\n",
    "def fix_encoding(tweet: str) -> str:\n",
    "    return ftfy.fix_encoding(tweet)\n",
    "\n",
    "def fix_repeated_letters(tweet: str) -> str:\n",
    "    \"\"\"Replace repeated characters (3 repetitions or more) with only two characters.\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "def fix_spelling(tweet: str) -> str:\n",
    "    \"\"\"Fix spelling error in tweets.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(tokenized)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] in misspelled:\n",
    "            tokenized[i] = spell.correction(tokenized[i])\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_mention,\n",
    "                 lower_case,\n",
    "                 remove_question_mark,\n",
    "                 remove_punctuation,\n",
    "                 remove_numbers,\n",
    "                 remove_whitespace,\n",
    "                 fix_repeated_letters,\n",
    "                 fix_encoding,\n",
    "                 fix_spelling\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling\n",
    "Preprocessing that should be done after spell correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet: str) -> str:\n",
    "    \"\"\"Remove stopwords from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if word not in stopWords])\n",
    "\n",
    "def stem_tweet(tweet: str) -> str:\n",
    "    tweet = nlp(tweet)\n",
    "    return ' '.join([token.lemma_ for token in tweet])\n",
    "\n",
    "\n",
    "def clean_tweet2(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_stopwords,\n",
    "                 stem_tweet\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df):\n",
    "    new_trans = 2\n",
    "    new_df = df.copy()\n",
    "    new_df['augmented'] = 0\n",
    "    for index, row in tqdm(new_df.iterrows()):\n",
    "        trans_en = translate(translate(row['content'], 'en', 'es'), 'es', 'en')\n",
    "\n",
    "        new_row_en = {'tweetid': row['tweetid'],\n",
    "                      'content': trans_en,\n",
    "                      'polarity': row['polarity'],\n",
    "                      'augmented': 1}\n",
    "\n",
    "        new_df = new_df.append(new_row_en, ignore_index=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = read_data(\"data/cr/intertass_cr_train.xml\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet)\n",
    "df.to_csv(\"data/cr/cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cr/cleaned.csv\")\n",
    "df = augment_data(df)\n",
    "df.to_csv(\"data/cr/cleaned_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cr/cleaned_augmented.csv\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet2)\n",
    "df = df.drop_duplicates('content', keep='first')\n",
    "df.to_csv(\"data/cr/cleaned_augmented_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_counts = Counter(df.polarity.values)\n",
    "plotdf = pd.DataFrame.from_dict(polarity_counts, orient='index')\n",
    "plotdf.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn tweets into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist([word for tweet in df.content for word in nltk.word_tokenize(tweet)])\n",
    "word_features = list(all_words) # [_document-classify-all-words]\n",
    "\n",
    "def document_features(document): # [_document-classify-extractor]\n",
    "    document_words = set(nltk.word_tokenize(document)) # [_document-classify-set]\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "        \n",
    "    text = Text(document)\n",
    "    text.language = 'es'\n",
    "    features['positive'] = sum([1 for w in text.words if w.polarity > 0])\n",
    "    features['neutral'] = sum([1 for w in text.words if w.polarity == 0])\n",
    "    features['negative'] = sum([1 for w in text.words if w.polarity < 0])\n",
    "    \n",
    "    return features\n",
    "df_sub = df\n",
    "featuresets = [(document_features(d), c) for (d,c) in zip(df_sub.content, df_sub.polarity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "kf = KFold(n_splits=n, shuffle=True)\n",
    "total = 0\n",
    "for train, test in kf.split(featuresets):\n",
    "    classifier = SklearnClassifier(LogisticRegression()).train(np.array(featuresets)[train])\n",
    "    total += nltk.classify.accuracy(classifier, (np.array(featuresets)[test]))\n",
    "print(\"Accuracy:\", total / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SklearnClassifier(LogisticRegression()).train(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/cr/test_cleaned.csv\")\n",
    "featuresets_test = [(document_features(d), c) for (d,c) in zip(df_test.content, df_test.polarity)]\n",
    "nltk.classify.accuracy(classifier, featuresets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TASS-2019",
   "language": "python",
   "name": "tass-2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
